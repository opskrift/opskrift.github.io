{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Opskrift is danish for recipe . This project aims to offer a collection of Python recipes mostly tailored for ML.","title":"Home"},{"location":"#home","text":"Opskrift is danish for recipe . This project aims to offer a collection of Python recipes mostly tailored for ML.","title":"Home"},{"location":"pytorch_reference/","text":"Refer to main/pytorch_reference for code examples. PyTorch data model several data sources can be made into a PyTorch Dataset object an item is (possibly transformed and) prepared in __getitem__ method a collate function will construct a batch finally, DataLoader wraps over a Dataset and emits batches of data","title":"Pytorch reference"},{"location":"pytorch_reference/#pytorch-data-model","text":"several data sources can be made into a PyTorch Dataset object an item is (possibly transformed and) prepared in __getitem__ method a collate function will construct a batch finally, DataLoader wraps over a Dataset and emits batches of data","title":"PyTorch data model"},{"location":"reference/augmented_pickle/","text":"Suppose you have some input data sources data_in on which you apply some process F parameterized by args : data_out = F(data_in, args) You want to serialize data_out , but also don't want to lose args , to preserve the exact setup that generated the output data. Now suppose you want to inspect args for a particular data_out : - Saving both {\"data\": data_out, \"args\": args} may not be a viable solution, as data_out needs to be fully loaded into memory without actually needing it. - Saving data_out and args separately necessitates extra care to keep them tied together. define a simple data format -- augmented pickle Pickle both objects, but read body on-demand: res = read_augmented_pickle(\"./data.apkl\", get_body=True) # get metadata (body is not loaded) meta = next(res) # query the generator again to get body (data) data = next(res) read_augmented_pickle ( path : str | PathLike , get_body : bool ) -> Iterable [ Any ] Read an augmented pickle file containing metadata and body . Returns a generator that can be queried on-demand using next . If get_body is False, only metadata is yielded. Source code in opskrift/augmented_pickle.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def read_augmented_pickle ( path : str | PathLike , get_body : bool , ) -> Iterable [ Any ]: \"\"\"Read an augmented pickle file containing `metadata` and `body`. Returns a generator that can be queried on-demand using `next`. If `get_body` is False, only `metadata` is yielded. \"\"\" with open ( path , \"rb\" ) as fp : metadata = pickle . load ( fp ) yield metadata if not get_body : return body = pickle . load ( fp ) yield body write_augmented_pickle ( metadata : Any , body : Any , path : str | PathLike ) -> None Write an augmented pickle file containing metadata and body . Source code in opskrift/augmented_pickle.py 38 39 40 41 42 43 44 45 46 def write_augmented_pickle ( metadata : Any , body : Any , path : str | PathLike , ) -> None : \"\"\"Write an augmented pickle file containing `metadata` and `body`.\"\"\" with open ( path , \"wb\" ) as fp : pickle . dump ( metadata , fp ) pickle . dump ( body , fp )","title":"Augmented pickle"},{"location":"reference/augmented_pickle/#opskrift.augmented_pickle.read_augmented_pickle","text":"Read an augmented pickle file containing metadata and body . Returns a generator that can be queried on-demand using next . If get_body is False, only metadata is yielded. Source code in opskrift/augmented_pickle.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def read_augmented_pickle ( path : str | PathLike , get_body : bool , ) -> Iterable [ Any ]: \"\"\"Read an augmented pickle file containing `metadata` and `body`. Returns a generator that can be queried on-demand using `next`. If `get_body` is False, only `metadata` is yielded. \"\"\" with open ( path , \"rb\" ) as fp : metadata = pickle . load ( fp ) yield metadata if not get_body : return body = pickle . load ( fp ) yield body","title":"read_augmented_pickle()"},{"location":"reference/augmented_pickle/#opskrift.augmented_pickle.write_augmented_pickle","text":"Write an augmented pickle file containing metadata and body . Source code in opskrift/augmented_pickle.py 38 39 40 41 42 43 44 45 46 def write_augmented_pickle ( metadata : Any , body : Any , path : str | PathLike , ) -> None : \"\"\"Write an augmented pickle file containing `metadata` and `body`.\"\"\" with open ( path , \"wb\" ) as fp : pickle . dump ( metadata , fp ) pickle . dump ( body , fp )","title":"write_augmented_pickle()"},{"location":"reference/config_utils/","text":"args_path_ensure_exists ( file_path : str ) -> Path Returns a Path object containing the resolved file_path . Raises FileNotFoundError if the path does not exist. Source code in opskrift/config_utils.py 8 9 10 11 12 13 14 15 16 17 def args_path_ensure_exists ( file_path : str ) -> Path : \"\"\"Returns a `Path` object containing the resolved `file_path`. Raises `FileNotFoundError` if the path does not exist. \"\"\" path = Path ( file_path ) . resolve () if not path . exists (): raise FileNotFoundError ( errno . ENOENT , os . strerror ( errno . ENOENT ), str ( path )) return path","title":"Config utils"},{"location":"reference/config_utils/#opskrift.config_utils.args_path_ensure_exists","text":"Returns a Path object containing the resolved file_path . Raises FileNotFoundError if the path does not exist. Source code in opskrift/config_utils.py 8 9 10 11 12 13 14 15 16 17 def args_path_ensure_exists ( file_path : str ) -> Path : \"\"\"Returns a `Path` object containing the resolved `file_path`. Raises `FileNotFoundError` if the path does not exist. \"\"\" path = Path ( file_path ) . resolve () if not path . exists (): raise FileNotFoundError ( errno . ENOENT , os . strerror ( errno . ENOENT ), str ( path )) return path","title":"args_path_ensure_exists()"},{"location":"reference/dict_utils/","text":"flatten ( obj : dict [ str , Any ], sep : str , name : str | None = None ) -> dict [ str , Any ] Flatten dictionary by combining nested keys with given separator. Source code in opskrift/dict_utils.py 6 7 8 9 10 11 12 13 14 15 16 17 def flatten ( obj : dict [ str , Any ], sep : str , name : str | None = None ) -> dict [ str , Any ]: \"\"\"Flatten dictionary by combining nested keys with given separator.\"\"\" items : list [ tuple [ str , Any ]] = [] for k , v in obj . items (): new_key = f \" { name }{ sep }{ k } \" if name else k if not isinstance ( v , dict ) or len ( v ) == 0 : items . append (( new_key , v )) else : items . extend ( flatten ( v , sep = sep , name = new_key ) . items ()) return dict ( items ) unflatten ( obj : dict [ str , Any ], sep : str ) -> dict [ str , Any ] Construct a nested dictionary by splitting keys on given separator. Source code in opskrift/dict_utils.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def unflatten ( obj : dict [ str , Any ], sep : str ) -> dict [ str , Any ]: \"\"\"Construct a nested dictionary by splitting keys on given separator.\"\"\" out : dict [ str , Any ] = {} for key , value in obj . items (): * init , last = key . split ( sep ) d = out for k in init : if k not in d : d [ k ] = {} d = d [ k ] d [ last ] = value return out","title":"Dict utils"},{"location":"reference/dict_utils/#opskrift.dict_utils.flatten","text":"Flatten dictionary by combining nested keys with given separator. Source code in opskrift/dict_utils.py 6 7 8 9 10 11 12 13 14 15 16 17 def flatten ( obj : dict [ str , Any ], sep : str , name : str | None = None ) -> dict [ str , Any ]: \"\"\"Flatten dictionary by combining nested keys with given separator.\"\"\" items : list [ tuple [ str , Any ]] = [] for k , v in obj . items (): new_key = f \" { name }{ sep }{ k } \" if name else k if not isinstance ( v , dict ) or len ( v ) == 0 : items . append (( new_key , v )) else : items . extend ( flatten ( v , sep = sep , name = new_key ) . items ()) return dict ( items )","title":"flatten()"},{"location":"reference/dict_utils/#opskrift.dict_utils.unflatten","text":"Construct a nested dictionary by splitting keys on given separator. Source code in opskrift/dict_utils.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def unflatten ( obj : dict [ str , Any ], sep : str ) -> dict [ str , Any ]: \"\"\"Construct a nested dictionary by splitting keys on given separator.\"\"\" out : dict [ str , Any ] = {} for key , value in obj . items (): * init , last = key . split ( sep ) d = out for k in init : if k not in d : d [ k ] = {} d = d [ k ] d [ last ] = value return out","title":"unflatten()"},{"location":"reference/gpu_utils/","text":"get_first_available_gpu_id ( limit : int ) -> Optional [ int ] Returns the ID of the first GPU with memory usage <= limit or None if no GPU could be found. Source code in opskrift/gpu_utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 def get_first_available_gpu_id ( limit : int ) -> Optional [ int ]: \"\"\"Returns the ID of the first GPU with memory usage <= limit or `None` if no GPU could be found.\"\"\" result = subprocess . check_output ( [ \"nvidia-smi\" , \"--query-gpu=memory.used\" , \"--format=csv,nounits,noheader\" ], encoding = \"utf-8\" , ) usage = np . array ([ int ( x ) for x in result . strip () . split ( \" \\n \" )]) try : return sorted ( np . where ( usage <= limit )[ 0 ])[ 0 ] except ValueError : print ( \"No GPU with 0 memory usage found!\" ) return None","title":"Gpu utils"},{"location":"reference/gpu_utils/#opskrift.gpu_utils.get_first_available_gpu_id","text":"Returns the ID of the first GPU with memory usage <= limit or None if no GPU could be found. Source code in opskrift/gpu_utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 def get_first_available_gpu_id ( limit : int ) -> Optional [ int ]: \"\"\"Returns the ID of the first GPU with memory usage <= limit or `None` if no GPU could be found.\"\"\" result = subprocess . check_output ( [ \"nvidia-smi\" , \"--query-gpu=memory.used\" , \"--format=csv,nounits,noheader\" ], encoding = \"utf-8\" , ) usage = np . array ([ int ( x ) for x in result . strip () . split ( \" \\n \" )]) try : return sorted ( np . where ( usage <= limit )[ 0 ])[ 0 ] except ValueError : print ( \"No GPU with 0 memory usage found!\" ) return None","title":"get_first_available_gpu_id()"},{"location":"reference/logger/","text":"Inspired by https://github.com/SebiSebi/friendlylog","title":"Logger"},{"location":"reference/maybe/","text":"Maybe Source code in opskrift/maybe.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class Maybe : def __init__ ( self , value : Optional [ a ], err : Optional [ Exception ] = None ): self . value = value self . err = err @classmethod def unit ( cls , * args , ** kwargs ) -> \"Maybe\" : return cls ( * args , ** kwargs ) def unwrap ( self ) -> a | Exception : if self . value is not None : assert self . err is None return self . value else : assert self . err is not None return self . err def bind ( self , func : Callable ) -> \"Maybe\" : \"\"\" Call `func` on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's `(>>=) :: Monad m => m a -> (a -> m b) -> m b` \"\"\" if self . value is None : return self try : return Maybe ( func ( self . value ), err = None ) except Exception as e : return Maybe ( value = None , err = e ) def __str__ ( self ): if self . value is None : if self . err is not None : return f \"Except ( { self . err } )\" else : return \"Nothing\" return f \"Just ( { self . value } )\" __repr__ = __str__ bind ( func : Callable ) -> 'Maybe' Call func on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's (>>=) :: Monad m => m a -> (a -> m b) -> m b Source code in opskrift/maybe.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def bind ( self , func : Callable ) -> \"Maybe\" : \"\"\" Call `func` on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's `(>>=) :: Monad m => m a -> (a -> m b) -> m b` \"\"\" if self . value is None : return self try : return Maybe ( func ( self . value ), err = None ) except Exception as e : return Maybe ( value = None , err = e )","title":"Maybe"},{"location":"reference/maybe/#opskrift.maybe.Maybe","text":"Source code in opskrift/maybe.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class Maybe : def __init__ ( self , value : Optional [ a ], err : Optional [ Exception ] = None ): self . value = value self . err = err @classmethod def unit ( cls , * args , ** kwargs ) -> \"Maybe\" : return cls ( * args , ** kwargs ) def unwrap ( self ) -> a | Exception : if self . value is not None : assert self . err is None return self . value else : assert self . err is not None return self . err def bind ( self , func : Callable ) -> \"Maybe\" : \"\"\" Call `func` on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's `(>>=) :: Monad m => m a -> (a -> m b) -> m b` \"\"\" if self . value is None : return self try : return Maybe ( func ( self . value ), err = None ) except Exception as e : return Maybe ( value = None , err = e ) def __str__ ( self ): if self . value is None : if self . err is not None : return f \"Except ( { self . err } )\" else : return \"Nothing\" return f \"Just ( { self . value } )\" __repr__ = __str__","title":"Maybe"},{"location":"reference/maybe/#opskrift.maybe.Maybe.bind","text":"Call func on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's (>>=) :: Monad m => m a -> (a -> m b) -> m b Source code in opskrift/maybe.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def bind ( self , func : Callable ) -> \"Maybe\" : \"\"\" Call `func` on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's `(>>=) :: Monad m => m a -> (a -> m b) -> m b` \"\"\" if self . value is None : return self try : return Maybe ( func ( self . value ), err = None ) except Exception as e : return Maybe ( value = None , err = e )","title":"bind()"},{"location":"reference/pipe/","text":"Modification of the https://github.com/JulienPalard/Pipe project to support arbitrary objects as input. See https://stackoverflow.com/a/69790644. EnterPipe Processing using pipes. Wrap input value in EnterPipe , chain processing functions, then mark the end of the pipe using ExitPipe . Parameters: Name Type Description Default obj Any The wrapped pipe input object. required Example: xs = np.array([1, 2, 3]) ys = EnterPipe(xs) | (lambda x: 2 * x) | np.median | ExitPipe print(ys) # 4.0 Source code in opskrift/pipe.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class EnterPipe : \"\"\"Processing using pipes. Wrap input value in `EnterPipe`, chain processing functions, then mark the end of the pipe using `ExitPipe`. Args: obj (Any): The wrapped pipe input object. Example: ``` xs = np.array([1, 2, 3]) ys = EnterPipe(xs) | (lambda x: 2 * x) | np.median | ExitPipe print(ys) # 4.0 ``` \"\"\" def __init__ ( self , obj : Any ): self . _object = obj def __or__ ( self , other ): if other is ExitPipe : return self . _object if not callable ( other ): raise TypeError ( f \"pipe element ' { other } ' is not callable\" ) return EnterPipe ( other ( self . _object ))","title":"Pipe"},{"location":"reference/pipe/#opskrift.pipe.EnterPipe","text":"Processing using pipes. Wrap input value in EnterPipe , chain processing functions, then mark the end of the pipe using ExitPipe . Parameters: Name Type Description Default obj Any The wrapped pipe input object. required Example: xs = np.array([1, 2, 3]) ys = EnterPipe(xs) | (lambda x: 2 * x) | np.median | ExitPipe print(ys) # 4.0 Source code in opskrift/pipe.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class EnterPipe : \"\"\"Processing using pipes. Wrap input value in `EnterPipe`, chain processing functions, then mark the end of the pipe using `ExitPipe`. Args: obj (Any): The wrapped pipe input object. Example: ``` xs = np.array([1, 2, 3]) ys = EnterPipe(xs) | (lambda x: 2 * x) | np.median | ExitPipe print(ys) # 4.0 ``` \"\"\" def __init__ ( self , obj : Any ): self . _object = obj def __or__ ( self , other ): if other is ExitPipe : return self . _object if not callable ( other ): raise TypeError ( f \"pipe element ' { other } ' is not callable\" ) return EnterPipe ( other ( self . _object ))","title":"EnterPipe"},{"location":"reference/singleton/","text":"","title":"Singleton"},{"location":"reference/train_utils/","text":"batchify ( data : np . ndarray , batch_size : int , func : Callable [[ np . ndarray ], np . ndarray ] | None = None ) -> Iterator [ np . ndarray ] Batchify data . If func is not None, then the emitted item is func(batch) . Parameters: Name Type Description Default data np . ndarray NumPy array of items to batchify. required batch_size int Batch size; must be between 1 and len(data) . required func Callable [[ np . ndarray ], np . ndarray ] Optional function to apply to each emitted batch. Defaults to identity function. None Returns: Type Description Iterator [ np . ndarray ] Iterator[np.ndarray]: Generator object containing batches. Source code in opskrift/train_utils.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def batchify ( data : np . ndarray , batch_size : int , func : Callable [[ np . ndarray ], np . ndarray ] | None = None , ) -> Iterator [ np . ndarray ]: \"\"\"Batchify `data`. If `func` is not None, then the emitted item is `func(batch)`. Args: data (np.ndarray): NumPy array of items to batchify. batch_size (int): Batch size; must be between 1 and `len(data)`. func (Callable[[np.ndarray], np.ndarray], optional): Optional function to apply to each emitted batch. Defaults to identity function. Returns: Iterator[np.ndarray]: Generator object containing batches. \"\"\" if not isinstance ( batch_size , int ) or not ( 1 <= batch_size <= len ( data )): raise ValueError ( f \"Batch size must be an int in [1, { data . shape [ 0 ] } ].\" ) if func is None : func = lambda x : x n = len ( data ) for i in range ( 0 , n , batch_size ): yield func ( data [ i : min ( i + batch_size , n )]) get_cosine_learning_rates ( lr_min : float , lr_max : float , freq : float , num_points : int ) -> list [ float ] Decay the learning rate based on a cosine schedule of frequency freq . Returns a list of N learning rate values in the interval [lr_min, lr_max] . Source code in opskrift/train_utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def get_cosine_learning_rates ( lr_min : float , lr_max : float , freq : float , num_points : int ) -> list [ float ]: \"\"\"Decay the learning rate based on a cosine schedule of frequency `freq`. Returns a list of `N` learning rate values in the interval `[lr_min, lr_max]`. \"\"\" lr = [] for i in range ( num_points ): freq = freq * i / num_points scaler = 0.5 * ( 1 + math . cos ( 2 * math . pi * freq )) # [0, 1] lr . append ( lr_min + scaler * ( lr_max - lr_min )) return lr split_data ( data : list [ Any ], train_f : float , test_f : float , shuffle : bool = False ) -> dict [ str , list [ Any ]] Get train / test / valid splits from data . If shuffle is True, then use a random permutation of data . valid split size is given by (1 - train_f - test_f) * len(data) . Parameters: Name Type Description Default data list [ Any ] Any collection of items to be split. required train_f float Train size factor from the entire length (must be between 0 and 1). required test_f float Test size factor from the entire length (must be between 0 and 1). required shuffle bool Whether to use a random permutation of data . False Returns: Type Description dict [ str , list [ Any ]] dict[str, list[Any]]: Keys are {train, test, valid}, and values are corresponding splits Source code in opskrift/train_utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def split_data ( data : list [ Any ], train_f : float , test_f : float , shuffle : bool = False ) -> dict [ str , list [ Any ]]: \"\"\"Get `train / test / valid` splits from `data`. If `shuffle` is True, then use a random permutation of `data`. `valid` split size is given by `(1 - train_f - test_f) * len(data)`. Args: data (list[Any]): Any collection of items to be split. train_f (float): Train size factor from the entire length (must be between 0 and 1). test_f (float): Test size factor from the entire length (must be between 0 and 1). shuffle (bool): Whether to use a random permutation of `data`. Returns: dict[str, list[Any]]: Keys are {train, test, valid}, and values are corresponding splits \"\"\" n = len ( data ) # use a generator to keep offset internally when taking elements if shuffle : rand_idx = np . random . permutation ( n ) gen = ( data [ i ] for i in rand_idx ) else : gen = ( x for x in data ) return { \"train\" : list ( itertoolz . take ( int ( n * train_f ), gen )), # take first \"test\" : list ( itertoolz . take ( int ( n * test_f ), gen )), # take next \"valid\" : list ( gen ), # take remaining }","title":"Train utils"},{"location":"reference/train_utils/#opskrift.train_utils.batchify","text":"Batchify data . If func is not None, then the emitted item is func(batch) . Parameters: Name Type Description Default data np . ndarray NumPy array of items to batchify. required batch_size int Batch size; must be between 1 and len(data) . required func Callable [[ np . ndarray ], np . ndarray ] Optional function to apply to each emitted batch. Defaults to identity function. None Returns: Type Description Iterator [ np . ndarray ] Iterator[np.ndarray]: Generator object containing batches. Source code in opskrift/train_utils.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def batchify ( data : np . ndarray , batch_size : int , func : Callable [[ np . ndarray ], np . ndarray ] | None = None , ) -> Iterator [ np . ndarray ]: \"\"\"Batchify `data`. If `func` is not None, then the emitted item is `func(batch)`. Args: data (np.ndarray): NumPy array of items to batchify. batch_size (int): Batch size; must be between 1 and `len(data)`. func (Callable[[np.ndarray], np.ndarray], optional): Optional function to apply to each emitted batch. Defaults to identity function. Returns: Iterator[np.ndarray]: Generator object containing batches. \"\"\" if not isinstance ( batch_size , int ) or not ( 1 <= batch_size <= len ( data )): raise ValueError ( f \"Batch size must be an int in [1, { data . shape [ 0 ] } ].\" ) if func is None : func = lambda x : x n = len ( data ) for i in range ( 0 , n , batch_size ): yield func ( data [ i : min ( i + batch_size , n )])","title":"batchify()"},{"location":"reference/train_utils/#opskrift.train_utils.get_cosine_learning_rates","text":"Decay the learning rate based on a cosine schedule of frequency freq . Returns a list of N learning rate values in the interval [lr_min, lr_max] . Source code in opskrift/train_utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def get_cosine_learning_rates ( lr_min : float , lr_max : float , freq : float , num_points : int ) -> list [ float ]: \"\"\"Decay the learning rate based on a cosine schedule of frequency `freq`. Returns a list of `N` learning rate values in the interval `[lr_min, lr_max]`. \"\"\" lr = [] for i in range ( num_points ): freq = freq * i / num_points scaler = 0.5 * ( 1 + math . cos ( 2 * math . pi * freq )) # [0, 1] lr . append ( lr_min + scaler * ( lr_max - lr_min )) return lr","title":"get_cosine_learning_rates()"},{"location":"reference/train_utils/#opskrift.train_utils.split_data","text":"Get train / test / valid splits from data . If shuffle is True, then use a random permutation of data . valid split size is given by (1 - train_f - test_f) * len(data) . Parameters: Name Type Description Default data list [ Any ] Any collection of items to be split. required train_f float Train size factor from the entire length (must be between 0 and 1). required test_f float Test size factor from the entire length (must be between 0 and 1). required shuffle bool Whether to use a random permutation of data . False Returns: Type Description dict [ str , list [ Any ]] dict[str, list[Any]]: Keys are {train, test, valid}, and values are corresponding splits Source code in opskrift/train_utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def split_data ( data : list [ Any ], train_f : float , test_f : float , shuffle : bool = False ) -> dict [ str , list [ Any ]]: \"\"\"Get `train / test / valid` splits from `data`. If `shuffle` is True, then use a random permutation of `data`. `valid` split size is given by `(1 - train_f - test_f) * len(data)`. Args: data (list[Any]): Any collection of items to be split. train_f (float): Train size factor from the entire length (must be between 0 and 1). test_f (float): Test size factor from the entire length (must be between 0 and 1). shuffle (bool): Whether to use a random permutation of `data`. Returns: dict[str, list[Any]]: Keys are {train, test, valid}, and values are corresponding splits \"\"\" n = len ( data ) # use a generator to keep offset internally when taking elements if shuffle : rand_idx = np . random . permutation ( n ) gen = ( data [ i ] for i in rand_idx ) else : gen = ( x for x in data ) return { \"train\" : list ( itertoolz . take ( int ( n * train_f ), gen )), # take first \"test\" : list ( itertoolz . take ( int ( n * test_f ), gen )), # take next \"valid\" : list ( gen ), # take remaining }","title":"split_data()"},{"location":"reference/viz/","text":"","title":"Viz"},{"location":"template/ADT/","text":"Simple example of Abstract Data Types in Python. In Haskell we would write data Result = OK Int | Failure String In Python we can do Result = Union[OK, Failure] where OK and Failure are frozen dataclasses.","title":"ADT"},{"location":"template/fault_tolerant_dataloader/","text":"","title":"Fault tolerant dataloader"},{"location":"template/mp_tqdm/","text":"Use tqdm in a multiprocessing environment. Each worker has its own progress bar whose position is given by the worker idx.","title":"Mp tqdm"}]}