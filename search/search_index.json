{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Opskrift is danish for recipe . This project aims to offer a collection of Python recipes mostly tailored for ML.","title":"Home"},{"location":"#home","text":"Opskrift is danish for recipe . This project aims to offer a collection of Python recipes mostly tailored for ML.","title":"Home"},{"location":"pytorch_reference/","text":"Refer to main/pytorch_reference for code examples. PyTorch data model several data sources can be made into a PyTorch Dataset object an item is (possibly transformed and) prepared in __getitem__ method a collate function will construct a batch finally, DataLoader wraps over a Dataset and emits batches of data","title":"Pytorch reference"},{"location":"pytorch_reference/#pytorch-data-model","text":"several data sources can be made into a PyTorch Dataset object an item is (possibly transformed and) prepared in __getitem__ method a collate function will construct a batch finally, DataLoader wraps over a Dataset and emits batches of data","title":"PyTorch data model"},{"location":"reference/augmented_pickle/","text":"Suppose you have some input data sources data_in on which you apply some process F parameterized by args : data_out = F(data_in, args) You want to serialize data_out , but also don't want to lose args , to preserve the exact setup that generated the output data. Now suppose you want to inspect args for a particular data_out : - Saving both {\"data\": data_out, \"args\": args} may not be a viable solution, as data_out needs to be fully loaded into memory without actually needing it. - Saving data_out and args separately necessitates extra care to keep them tied together. Solution: define a simple data format -- augmented pickle <metadata> <body (actual data)> Pickle both objects, but read body on-demand: res = read_augmented_pickle(\"./data.apkl\", get_body=True) # get metadata (body is not loaded) meta = next(res) # query the generator again to get body (data) data = next(res) read_augmented_pickle ( path : Union [ str , os . PathLike ], get_body : bool ) -> Iterable [ Any ] Read an augmented pickle file containing metadata and body . Returns a generator that can be queried on-demand using next . If get_body is False, only metadata is yielded. Source code in src/augmented_pickle.py def read_augmented_pickle ( path : Union [ str , PathLike ], get_body : bool , ) -> Iterable [ Any ]: \"\"\"Read an augmented pickle file containing `metadata` and `body`. Returns a generator that can be queried on-demand using `next`. If `get_body` is False, only `metadata` is yielded. \"\"\" with open ( path , \"rb\" ) as fp : metadata = pickle . load ( fp ) yield metadata if not get_body : return body = pickle . load ( fp ) yield body write_augmented_pickle ( metadata : Any , body : Any , path : Union [ str , os . PathLike ]) -> None Write an augmented pickle file containing metadata and body . Source code in src/augmented_pickle.py def write_augmented_pickle ( metadata : Any , body : Any , path : Union [ str , PathLike ], ) -> None : \"\"\"Write an augmented pickle file containing `metadata` and `body`.\"\"\" with open ( path , \"wb\" ) as fp : pickle . dump ( metadata , fp ) pickle . dump ( body , fp )","title":"Augmented pickle"},{"location":"reference/augmented_pickle/#src.augmented_pickle.read_augmented_pickle","text":"Read an augmented pickle file containing metadata and body . Returns a generator that can be queried on-demand using next . If get_body is False, only metadata is yielded. Source code in src/augmented_pickle.py def read_augmented_pickle ( path : Union [ str , PathLike ], get_body : bool , ) -> Iterable [ Any ]: \"\"\"Read an augmented pickle file containing `metadata` and `body`. Returns a generator that can be queried on-demand using `next`. If `get_body` is False, only `metadata` is yielded. \"\"\" with open ( path , \"rb\" ) as fp : metadata = pickle . load ( fp ) yield metadata if not get_body : return body = pickle . load ( fp ) yield body","title":"read_augmented_pickle()"},{"location":"reference/augmented_pickle/#src.augmented_pickle.write_augmented_pickle","text":"Write an augmented pickle file containing metadata and body . Source code in src/augmented_pickle.py def write_augmented_pickle ( metadata : Any , body : Any , path : Union [ str , PathLike ], ) -> None : \"\"\"Write an augmented pickle file containing `metadata` and `body`.\"\"\" with open ( path , \"wb\" ) as fp : pickle . dump ( metadata , fp ) pickle . dump ( body , fp )","title":"write_augmented_pickle()"},{"location":"reference/config_utils/","text":"args_path_ensure_exists ( file_path : str ) -> Path Returns a Path object containing the resolved file_path . Raises FileNotFoundError if the path does not exist. Source code in src/config_utils.py def args_path_ensure_exists ( file_path : str ) -> Path : \"\"\"Returns a `Path` object containing the resolved `file_path`. Raises `FileNotFoundError` if the path does not exist. \"\"\" path = Path ( file_path ) . resolve () if not path . exists (): raise FileNotFoundError ( errno . ENOENT , os . strerror ( errno . ENOENT ), str ( path )) return path","title":"Config utils"},{"location":"reference/config_utils/#src.config_utils.args_path_ensure_exists","text":"Returns a Path object containing the resolved file_path . Raises FileNotFoundError if the path does not exist. Source code in src/config_utils.py def args_path_ensure_exists ( file_path : str ) -> Path : \"\"\"Returns a `Path` object containing the resolved `file_path`. Raises `FileNotFoundError` if the path does not exist. \"\"\" path = Path ( file_path ) . resolve () if not path . exists (): raise FileNotFoundError ( errno . ENOENT , os . strerror ( errno . ENOENT ), str ( path )) return path","title":"args_path_ensure_exists()"},{"location":"reference/dict_utils/","text":"flatten ( obj : MutableMapping , sep : str , name : str = None ) -> MutableMapping Flatten dictionary by combining nested keys with given separator. Source code in src/dict_utils.py def flatten ( obj : MutableMapping , sep : str , name : str = None ) -> MutableMapping : \"\"\"Flatten dictionary by combining nested keys with given separator.\"\"\" items : List [ Tuple [ str , Any ]] = [] for k , v in obj . items (): new_key = name + sep + k if name else k if isinstance ( v , MutableMapping ): items . extend ( flatten ( v , sep = sep , name = new_key ) . items ()) else : items . append (( new_key , v )) return dict ( items ) unflatten ( obj : Dict [ str , Any ], sep : str ) -> Dict [ Any , Any ] Construct a nested dictionary by splitting keys on given separator. Source code in src/dict_utils.py def unflatten ( obj : Dict [ str , Any ], sep : str ) -> Dict [ Any , Any ]: \"\"\"Construct a nested dictionary by splitting keys on given separator.\"\"\" out : Dict [ str , Any ] = {} for key , value in obj . items (): * init , last = key . split ( sep ) d = out for k in init : if k not in d : d [ k ] = {} d = d [ k ] d [ last ] = value return out","title":"Dict utils"},{"location":"reference/dict_utils/#src.dict_utils.flatten","text":"Flatten dictionary by combining nested keys with given separator. Source code in src/dict_utils.py def flatten ( obj : MutableMapping , sep : str , name : str = None ) -> MutableMapping : \"\"\"Flatten dictionary by combining nested keys with given separator.\"\"\" items : List [ Tuple [ str , Any ]] = [] for k , v in obj . items (): new_key = name + sep + k if name else k if isinstance ( v , MutableMapping ): items . extend ( flatten ( v , sep = sep , name = new_key ) . items ()) else : items . append (( new_key , v )) return dict ( items )","title":"flatten()"},{"location":"reference/dict_utils/#src.dict_utils.unflatten","text":"Construct a nested dictionary by splitting keys on given separator. Source code in src/dict_utils.py def unflatten ( obj : Dict [ str , Any ], sep : str ) -> Dict [ Any , Any ]: \"\"\"Construct a nested dictionary by splitting keys on given separator.\"\"\" out : Dict [ str , Any ] = {} for key , value in obj . items (): * init , last = key . split ( sep ) d = out for k in init : if k not in d : d [ k ] = {} d = d [ k ] d [ last ] = value return out","title":"unflatten()"},{"location":"reference/gpu_utils/","text":"get_first_available_gpu_id ( limit : int ) -> Optional [ int ] Returns the ID of the first GPU with memory usage <= limit or None if no GPU could be found. Source code in src/gpu_utils.py def get_first_available_gpu_id ( limit : int ) -> Optional [ int ]: \"\"\"Returns the ID of the first GPU with memory usage <= limit or `None` if no GPU could be found.\"\"\" result = subprocess . check_output ( [ \"nvidia-smi\" , \"--query-gpu=memory.used\" , \"--format=csv,nounits,noheader\" ], encoding = \"utf-8\" , ) usage = np . array ([ int ( x ) for x in result . strip () . split ( \" \\n \" )]) try : return sorted ( np . where ( usage <= limit )[ 0 ])[ 0 ] except ValueError : print ( \"No GPU with 0 memory usage found!\" ) return None","title":"Gpu utils"},{"location":"reference/gpu_utils/#src.gpu_utils.get_first_available_gpu_id","text":"Returns the ID of the first GPU with memory usage <= limit or None if no GPU could be found. Source code in src/gpu_utils.py def get_first_available_gpu_id ( limit : int ) -> Optional [ int ]: \"\"\"Returns the ID of the first GPU with memory usage <= limit or `None` if no GPU could be found.\"\"\" result = subprocess . check_output ( [ \"nvidia-smi\" , \"--query-gpu=memory.used\" , \"--format=csv,nounits,noheader\" ], encoding = \"utf-8\" , ) usage = np . array ([ int ( x ) for x in result . strip () . split ( \" \\n \" )]) try : return sorted ( np . where ( usage <= limit )[ 0 ])[ 0 ] except ValueError : print ( \"No GPU with 0 memory usage found!\" ) return None","title":"get_first_available_gpu_id()"},{"location":"reference/logger/","text":"Inspired by https://github.com/SebiSebi/friendlylog","title":"Logger"},{"location":"reference/maybe/","text":"Maybe __repr__ ( self ) special Return str(self). Source code in src/maybe.py def __str__ ( self ): if self . value is None : if self . err is not None : return f \"Except ( { self . err } )\" else : return \"Nothing\" return f \"Just ( { self . value } )\" bind ( self , func : Callable [[ ~ a ], Optional [ ~ b ]]) -> Optional [ ~ b ] Call func on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's (>>=) :: Monad m => m a -> (a -> m b) -> m b Source code in src/maybe.py def bind ( self , func : Callable [[ a ], Optional [ b ]]) -> Optional [ b ]: \"\"\" Call `func` on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's `(>>=) :: Monad m => m a -> (a -> m b) -> m b` \"\"\" if self . value is None : return self try : return Maybe ( func ( self . value ), err = None ) except Exception as e : return Maybe ( value = None , err = e )","title":"Maybe"},{"location":"reference/maybe/#src.maybe.Maybe","text":"","title":"Maybe"},{"location":"reference/maybe/#src.maybe.Maybe.__repr__","text":"Return str(self). Source code in src/maybe.py def __str__ ( self ): if self . value is None : if self . err is not None : return f \"Except ( { self . err } )\" else : return \"Nothing\" return f \"Just ( { self . value } )\"","title":"__repr__()"},{"location":"reference/maybe/#src.maybe.Maybe.bind","text":"Call func on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's (>>=) :: Monad m => m a -> (a -> m b) -> m b Source code in src/maybe.py def bind ( self , func : Callable [[ a ], Optional [ b ]]) -> Optional [ b ]: \"\"\" Call `func` on the wrapped value, storing and propagating the exception if any. This has the effect of \"fail-on-first-error\". Similar to Haskell's `(>>=) :: Monad m => m a -> (a -> m b) -> m b` \"\"\" if self . value is None : return self try : return Maybe ( func ( self . value ), err = None ) except Exception as e : return Maybe ( value = None , err = e )","title":"bind()"},{"location":"reference/singleton/","text":"","title":"Singleton"},{"location":"reference/train_utils/","text":"batchify ( data : ndarray , batch_size : int , func : Callable [[ numpy . ndarray ], numpy . ndarray ] = None ) -> Iterator [ numpy . ndarray ] Batchify data . If func is not None, then the emitted item is func(batch) . Parameters: Name Type Description Default data np.ndarray NumPy array of items to batchify. required batch_size int Batch size; must be between 1 and len(data) . required func Callable[[np.ndarray], np.ndarray] Optional function to apply to each emitted batch. Defaults to identity function. None Returns: Type Description Iterator[np.ndarray] Generator object containing batches. Source code in src/train_utils.py def batchify ( data : np . ndarray , batch_size : int , func : Callable [[ np . ndarray ], np . ndarray ] = None ) -> Iterator [ np . ndarray ]: \"\"\"Batchify `data`. If `func` is not None, then the emitted item is `func(batch)`. Args: data (np.ndarray): NumPy array of items to batchify. batch_size (int): Batch size; must be between 1 and `len(data)`. func (Callable[[np.ndarray], np.ndarray], optional): Optional function to apply to each emitted batch. Defaults to identity function. Returns: Iterator[np.ndarray]: Generator object containing batches. \"\"\" if not isinstance ( batch_size , int ) or not ( 1 <= batch_size <= len ( data )): raise ValueError ( f \"Batch size must be an int in [1, { data . shape [ 0 ] } ].\" ) if func is None : func = lambda x : x n = len ( data ) for i in range ( 0 , n , batch_size ): yield func ( data [ i : min ( i + batch_size , n )]) get_cosine_learning_rates ( lr_min : float , lr_max : float , f : float , N : int ) Decay the learning rate based on a cosine schedule of frequency f . Returns a list of N learning rate values in the interval [lr_min, lr_max] . Source code in src/train_utils.py def get_cosine_learning_rates ( lr_min : float , lr_max : float , f : float , N : int ): \"\"\"Decay the learning rate based on a cosine schedule of frequency `f`. Returns a list of `N` learning rate values in the interval `[lr_min, lr_max]`. \"\"\" lr = [] for i in range ( N ): freq = f * i / N scaler = 0.5 * ( 1 + math . cos ( 2 * math . pi * freq )) # [0, 1] l = lr_min + scaler * ( lr_max - lr_min ) lr . append ( l ) return lr split_data ( data : List [ Any ], train_f : float , test_f : float , shuffle : bool = False ) -> Dict [ str , List [ Any ]] Get train / test / valid splits from data . If shuffle is True, then use a random permutation of data . valid split size is given by (1 - train_f - test_f) * len(data) . Parameters: Name Type Description Default data List[Any] Any collection of items to be split. required train_f float Train size factor from the entire length (must be between 0 and 1). required test_f float Test size factor from the entire length (must be between 0 and 1). required shuffle bool Whether to use a random permutation of data . False Returns: Type Description Dict[str, List[Any]] Keys are {train, test, valid}, and values are corresponding splits Source code in src/train_utils.py def split_data ( data : List [ Any ], train_f : float , test_f : float , shuffle : bool = False ) -> Dict [ str , List [ Any ]]: \"\"\"Get `train / test / valid` splits from `data`. If `shuffle` is True, then use a random permutation of `data`. `valid` split size is given by `(1 - train_f - test_f) * len(data)`. Args: data (List[Any]): Any collection of items to be split. train_f (float): Train size factor from the entire length (must be between 0 and 1). test_f (float): Test size factor from the entire length (must be between 0 and 1). shuffle (bool): Whether to use a random permutation of `data`. Returns: Dict[str, List[Any]]: Keys are {train, test, valid}, and values are corresponding splits \"\"\" n = len ( data ) # use a generator to keep offset internally when taking elements if shuffle : rand_idx = np . random . permutation ( n ) gen = ( data [ i ] for i in rand_idx ) else : gen = ( x for x in data ) return { \"train\" : list ( itertoolz . take ( int ( n * train_f ), gen )), # take first \"test\" : list ( itertoolz . take ( int ( n * test_f ), gen )), # take next \"valid\" : list ( gen ), # take remaining }","title":"Train utils"},{"location":"reference/train_utils/#src.train_utils.batchify","text":"Batchify data . If func is not None, then the emitted item is func(batch) . Parameters: Name Type Description Default data np.ndarray NumPy array of items to batchify. required batch_size int Batch size; must be between 1 and len(data) . required func Callable[[np.ndarray], np.ndarray] Optional function to apply to each emitted batch. Defaults to identity function. None Returns: Type Description Iterator[np.ndarray] Generator object containing batches. Source code in src/train_utils.py def batchify ( data : np . ndarray , batch_size : int , func : Callable [[ np . ndarray ], np . ndarray ] = None ) -> Iterator [ np . ndarray ]: \"\"\"Batchify `data`. If `func` is not None, then the emitted item is `func(batch)`. Args: data (np.ndarray): NumPy array of items to batchify. batch_size (int): Batch size; must be between 1 and `len(data)`. func (Callable[[np.ndarray], np.ndarray], optional): Optional function to apply to each emitted batch. Defaults to identity function. Returns: Iterator[np.ndarray]: Generator object containing batches. \"\"\" if not isinstance ( batch_size , int ) or not ( 1 <= batch_size <= len ( data )): raise ValueError ( f \"Batch size must be an int in [1, { data . shape [ 0 ] } ].\" ) if func is None : func = lambda x : x n = len ( data ) for i in range ( 0 , n , batch_size ): yield func ( data [ i : min ( i + batch_size , n )])","title":"batchify()"},{"location":"reference/train_utils/#src.train_utils.get_cosine_learning_rates","text":"Decay the learning rate based on a cosine schedule of frequency f . Returns a list of N learning rate values in the interval [lr_min, lr_max] . Source code in src/train_utils.py def get_cosine_learning_rates ( lr_min : float , lr_max : float , f : float , N : int ): \"\"\"Decay the learning rate based on a cosine schedule of frequency `f`. Returns a list of `N` learning rate values in the interval `[lr_min, lr_max]`. \"\"\" lr = [] for i in range ( N ): freq = f * i / N scaler = 0.5 * ( 1 + math . cos ( 2 * math . pi * freq )) # [0, 1] l = lr_min + scaler * ( lr_max - lr_min ) lr . append ( l ) return lr","title":"get_cosine_learning_rates()"},{"location":"reference/train_utils/#src.train_utils.split_data","text":"Get train / test / valid splits from data . If shuffle is True, then use a random permutation of data . valid split size is given by (1 - train_f - test_f) * len(data) . Parameters: Name Type Description Default data List[Any] Any collection of items to be split. required train_f float Train size factor from the entire length (must be between 0 and 1). required test_f float Test size factor from the entire length (must be between 0 and 1). required shuffle bool Whether to use a random permutation of data . False Returns: Type Description Dict[str, List[Any]] Keys are {train, test, valid}, and values are corresponding splits Source code in src/train_utils.py def split_data ( data : List [ Any ], train_f : float , test_f : float , shuffle : bool = False ) -> Dict [ str , List [ Any ]]: \"\"\"Get `train / test / valid` splits from `data`. If `shuffle` is True, then use a random permutation of `data`. `valid` split size is given by `(1 - train_f - test_f) * len(data)`. Args: data (List[Any]): Any collection of items to be split. train_f (float): Train size factor from the entire length (must be between 0 and 1). test_f (float): Test size factor from the entire length (must be between 0 and 1). shuffle (bool): Whether to use a random permutation of `data`. Returns: Dict[str, List[Any]]: Keys are {train, test, valid}, and values are corresponding splits \"\"\" n = len ( data ) # use a generator to keep offset internally when taking elements if shuffle : rand_idx = np . random . permutation ( n ) gen = ( data [ i ] for i in rand_idx ) else : gen = ( x for x in data ) return { \"train\" : list ( itertoolz . take ( int ( n * train_f ), gen )), # take first \"test\" : list ( itertoolz . take ( int ( n * test_f ), gen )), # take next \"valid\" : list ( gen ), # take remaining }","title":"split_data()"},{"location":"reference/viz/","text":"","title":"Viz"},{"location":"template/_ADT/","text":"Simple example of Abstract Data Types in Python. In Haskell we would write: data Result = OK Int | Failure String In Python we can do: Result = Union[OK, Failure] where OK and Failure are frozen dataclasses. Failure dataclass Failure(msg: str) OK dataclass OK(result: int)","title":" ADT"},{"location":"template/_ADT/#src._ADT.Failure","text":"Failure(msg: str)","title":"Failure"},{"location":"template/_ADT/#src._ADT.OK","text":"OK(result: int)","title":"OK"},{"location":"template/_fault_tolerant_dataloader/","text":"","title":" fault tolerant dataloader"},{"location":"template/_mp_tqdm/","text":"Use tqdm in a multiprocessing environment. Each worker has its own progress bar whose position is given by the worker idx.","title":" mp tqdm"},{"location":"template/_mp_wrapper/","text":"processing_func ( x , data , foo , bar ) -> Dict [ str , List [ int ]] This function does not know anything about CLI args. Source code in src/_mp_wrapper.py def processing_func ( x , data , foo , bar ) -> OutputType : \"\"\" This function does not know anything about CLI args. \"\"\" out = { \"foo\" : [ 1 , 2 , 3 ]} return out","title":" mp wrapper"},{"location":"template/_mp_wrapper/#src._mp_wrapper.processing_func","text":"This function does not know anything about CLI args. Source code in src/_mp_wrapper.py def processing_func ( x , data , foo , bar ) -> OutputType : \"\"\" This function does not know anything about CLI args. \"\"\" out = { \"foo\" : [ 1 , 2 , 3 ]} return out","title":"processing_func()"},{"location":"template/_processing_script/","text":"","title":" processing script"}]}